{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3326124",
   "metadata": {},
   "source": [
    "# Project 8: Deploy a model with a big data architecture in AWS\n",
    "\n",
    "*Pierre-Eloi Ragetly*\n",
    "\n",
    "This notebook has been realised to perform a dimension reduction on an image dataset with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525edd1b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Load-images\" data-toc-modified-id=\"Load-images-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load images</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-all-subfolders\" data-toc-modified-id=\"Get-all-subfolders-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get all subfolders</a></span></li><li><span><a href=\"#Create-a-DataFrame-with-all-images\" data-toc-modified-id=\"Create-a-DataFrame-with-all-images-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Create a DataFrame with all images</a></span></li></ul></li><li><span><a href=\"#Create-a-Category-column\" data-toc-modified-id=\"Create-a-Category-column-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create a Category column</a></span></li><li><span><a href=\"#Feature-extraction\" data-toc-modified-id=\"Feature-extraction-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-model\" data-toc-modified-id=\"Prepare-model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Prepare model</a></span></li><li><span><a href=\"#Prepare-data\" data-toc-modified-id=\"Prepare-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Prepare data</a></span></li><li><span><a href=\"#Define-featurization-in-a-Pandas-UDF\" data-toc-modified-id=\"Define-featurization-in-a-Pandas-UDF-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Define featurization in a Pandas UDF</a></span></li><li><span><a href=\"#Apply-featurization-to-the-DataFrame-of-images\" data-toc-modified-id=\"Apply-featurization-to-the-DataFrame-of-images-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Apply featurization to the DataFrame of images</a></span></li></ul></li><li><span><a href=\"#Save-results\" data-toc-modified-id=\"Save-results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Save results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e14e41",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a47f7f",
   "metadata": {},
   "source": [
    "First, let's import a common modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f74293e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:24.551067Z",
     "start_time": "2022-11-18T14:06:21.112535Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:06:21.499346: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "# Import numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# image preprocessing\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Import deep learning models with tensorflow\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "#from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "# Import pyspark library\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4260af2",
   "metadata": {},
   "source": [
    "Let's create a spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f920e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:29.480244Z",
     "start_time": "2022-11-18T14:06:26.944631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 15:06:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"Warn\")\n",
    "spark = SparkSession.builder.appName('ImageReduction').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b29f22",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7b80f",
   "metadata": {},
   "source": [
    "### Get all subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc56aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:33.392286Z",
     "start_time": "2022-11-18T14:06:33.376000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/sample_dataset/Apple_Golden_3', 'dataset/sample_dataset/Cherry_1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = 'dataset/sample_dataset'\n",
    "subfolders = [d.path for d in os.scandir(root_path) if d.is_dir()]\n",
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e158bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:34.402388Z",
     "start_time": "2022-11-18T14:06:34.399673Z"
    }
   },
   "outputs": [],
   "source": [
    "# change all ' ' by '_' in directory names\n",
    "# to avoid loading issues with pyspark\n",
    "for d in subfolders:\n",
    "    os.rename(d, d.replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e52ad",
   "metadata": {},
   "source": [
    "### Create a DataFrame with all images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef42fd",
   "metadata": {},
   "source": [
    "Since Spark 2.4, reading image in compressed formats (jpg, png, etc...) is possible with `spark.read.format('image').load('path')`.\n",
    "\n",
    "The image is read with the ImageIO *Java Library*, and has a special DataFrame schema. The schema contains a StructType Column \"Image\" with all information about reading data.\n",
    "- origin: `StringType` *image file path* \n",
    "- height: `IntegerType` *image height*\n",
    "- width: `IntegerType` *image width*\n",
    "- nChannels: `IntegerType` *number of image channels*\n",
    "- mode: `IntegerType` *OpenCV-compatible type* \n",
    "- data: `BinaryType` *Image bytes in OpenCV-compatible order (BGR)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd90adc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:38.219262Z",
     "start_time": "2022-11-18T14:06:38.215986Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_img(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .jpg images saved in a directory to a binary Spark DataFrame.\n",
    "    \"\"\"\n",
    "    images = spark.read.format(\"binaryFile\") \\\n",
    "        .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "        .option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .load(dir_path)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397f29f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:42.045625Z",
     "start_time": "2022-11-18T14:06:40.376168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "dataframes =[load_img(dir_path) for dir_path in subfolders]\n",
    "\n",
    "# merge DataFrames\n",
    "image_df = reduce(lambda first, second: first.union(second), dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e9e6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:06:44.408931Z",
     "start_time": "2022-11-18T14:06:44.400001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ac2f2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T22:01:50.712027Z",
     "start_time": "2022-11-17T22:01:48.978147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "dataframes =[spark.read.format('image').load(p) for p in subfolders]\n",
    "\n",
    "# merge DataFrames\n",
    "image_df = reduce(lambda first, second: first.union(second), dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc72791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T22:01:50.722901Z",
     "start_time": "2022-11-17T22:01:50.713534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796d0a7",
   "metadata": {},
   "source": [
    "## Create a Category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389f5cc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:07:19.101424Z",
     "start_time": "2022-11-18T14:07:17.593148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+------------+\n",
      "|                path|   modificationTime|length|             content|    category|\n",
      "+--------------------+-------------------+------+--------------------+------------+\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4869|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4865|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4857|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4847|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4847|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4842|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4834|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4824|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4820|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4815|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "+--------------------+-------------------+------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex = r'(.*)/(.*[a-zA-Z])(.*)/'\n",
    "df = image_df.withColumn('category', regexp_extract('path', regex, 2))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93782b02",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba536e",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72b882",
   "metadata": {},
   "source": [
    "We will use the **InceptionV3** model to extract features from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "# model: InceptionV3\n",
    "# extracting feature from images\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                                 outputCol=\"features\",\n",
    "                                 modelName=\"InceptionV3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1600827",
   "metadata": {},
   "source": [
    "broadcast explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6182c19c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:08:29.334499Z",
     "start_time": "2022-11-18T14:08:27.587020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:08:27.604835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#model = InceptionV3(include_top=False)\n",
    "model = ResNet50(include_top=False)\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed\n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = ResNet50(weights=None, include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c73a3",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d7a858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:08:54.998033Z",
     "start_time": "2022-11-18T14:08:54.995385Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_img(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7237f",
   "metadata": {},
   "source": [
    "### Define featurization in a Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42bbb6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:08:58.950548Z",
     "start_time": "2022-11-18T14:08:58.947633Z"
    }
   },
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    For some layers, output features will be multi-dimensional tensors.\n",
    "    Feature tensors are flattened to vectors for easier storage in Spark DataFrames.\n",
    "    -----------    \n",
    "    Return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    X = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(X)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9aace0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:10:38.689381Z",
     "start_time": "2022-11-18T14:10:37.687455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierre-eloiragetly/.local/share/virtualenvs/OC_DS_Project8-hpBRnWRl/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    \"\"\"\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns\n",
    "    a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    With Sclaar Iterator pandas UDFS,\n",
    "    the model can be loaded once and re-used after for multiple data batches.\n",
    "    This amortizes the overhead of loading big models.\n",
    "    -----------\n",
    "    Parameters:\n",
    "    content_series_iter: This argument is an iterator over batches of data,\n",
    "                         where each batch is a pandas Series of image data.\n",
    "    -----------\n",
    "    Return: a Spark DataFrame column of type ArrayType(FloatType)\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5141c7",
   "metadata": {},
   "source": [
    "### Apply featurization to the DataFrame of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5da10",
   "metadata": {},
   "source": [
    "Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors. It can be avoided by reducing the Arrow batch size via `maxRecordsPerBatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b29aa59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:10:44.106498Z",
     "start_time": "2022-11-18T14:10:44.102977Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27303cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:11:47.325241Z",
     "start_time": "2022-11-18T14:11:46.112872Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 15:11:47 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 394, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'keras'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "22/11/18 15:11:47 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (imac.home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 394, in read_udfs\n",
      "    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'keras'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "22/11/18 15:11:47 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 394, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'keras'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m features_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, featurize_udf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m)) \\\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mfeatures_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/OC_DS_Project8-hpBRnWRl/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/OC_DS_Project8-hpBRnWRl/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/OC_DS_Project8-hpBRnWRl/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 394, in read_udfs\n    arg_offsets, udf = read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=0)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n  File \"/Users/pierre-eloiragetly/spark/spark-3.3.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'keras'\n"
     ]
    }
   ],
   "source": [
    "features_df = df.withColumn('features', featurize_udf('path')) \\\n",
    "                .select('path', 'category', 'content', 'features')\n",
    "\n",
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d6c06",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede2c78",
   "metadata": {},
   "source": [
    "Results will be saved using the parquet format for performance purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2142ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(\"path_s3_buquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
