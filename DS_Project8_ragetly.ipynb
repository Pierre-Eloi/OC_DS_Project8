{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3326124",
   "metadata": {},
   "source": [
    "# Project 8: Deploy a model with a big data architecture in AWS\n",
    "\n",
    "*Pierre-Eloi Ragetly*\n",
    "\n",
    "This notebook has been realised to perform a dimension reduction on an image dataset with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525edd1b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Load-images\" data-toc-modified-id=\"Load-images-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load images</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-all-subfolders\" data-toc-modified-id=\"Get-all-subfolders-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Get all subfolders</a></span></li><li><span><a href=\"#Create-a-DataFrame-with-all-images\" data-toc-modified-id=\"Create-a-DataFrame-with-all-images-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Create a DataFrame with all images</a></span></li></ul></li><li><span><a href=\"#Create-a-Category-column\" data-toc-modified-id=\"Create-a-Category-column-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create a Category column</a></span></li><li><span><a href=\"#Feature-extraction\" data-toc-modified-id=\"Feature-extraction-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-model\" data-toc-modified-id=\"Prepare-model-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Prepare model</a></span></li><li><span><a href=\"#Prepare-data\" data-toc-modified-id=\"Prepare-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Prepare data</a></span></li><li><span><a href=\"#Define-featurization-in-a-Pandas-UDF\" data-toc-modified-id=\"Define-featurization-in-a-Pandas-UDF-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Define featurization in a Pandas UDF</a></span></li><li><span><a href=\"#Apply-featurization-to-the-DataFrame-of-images\" data-toc-modified-id=\"Apply-featurization-to-the-DataFrame-of-images-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Apply featurization to the DataFrame of images</a></span></li></ul></li><li><span><a href=\"#Save-results\" data-toc-modified-id=\"Save-results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Save results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e14e41",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a47f7f",
   "metadata": {},
   "source": [
    "First, let's import a common modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f74293e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:22.963354Z",
     "start_time": "2022-11-18T14:52:22.958981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "# Import numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# image preprocessing\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Import deep learning models with tensorflow\n",
    "#from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "# Import pyspark library\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4260af2",
   "metadata": {},
   "source": [
    "Let's create a spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f920e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:27.291505Z",
     "start_time": "2022-11-18T14:52:27.286620Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"Warn\")\n",
    "spark = SparkSession.builder.appName('ImageReduction').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b29f22",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7b80f",
   "metadata": {},
   "source": [
    "### Get all subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbc56aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:29.954593Z",
     "start_time": "2022-11-18T14:52:29.950879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/sample_dataset/Apple_Golden_3', 'dataset/sample_dataset/Cherry_1']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = 'dataset/sample_dataset'\n",
    "subfolders = [d.path for d in os.scandir(root_path) if d.is_dir()]\n",
    "subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e158bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:30.952525Z",
     "start_time": "2022-11-18T14:52:30.950119Z"
    }
   },
   "outputs": [],
   "source": [
    "# change all ' ' by '_' in directory names\n",
    "# to avoid loading issues with pyspark\n",
    "for d in subfolders:\n",
    "    os.rename(d, d.replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e52ad",
   "metadata": {},
   "source": [
    "### Create a DataFrame with all images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef42fd",
   "metadata": {},
   "source": [
    "**TO BE RE-WRITTEN**\n",
    "\n",
    "\n",
    "Since Spark 2.4, reading image in compressed formats (jpg, png, etc...) is possible with `spark.read.format('image').load('path')`.\n",
    "\n",
    "The image is read with the ImageIO *Java Library*, and has a special DataFrame schema. The schema contains a StructType Column \"Image\" with all information about reading data.\n",
    "- origin: `StringType` *image file path* \n",
    "- height: `IntegerType` *image height*\n",
    "- width: `IntegerType` *image width*\n",
    "- nChannels: `IntegerType` *number of image channels*\n",
    "- mode: `IntegerType` *OpenCV-compatible type* \n",
    "- data: `BinaryType` *Image bytes in OpenCV-compatible order (BGR)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd90adc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:34.009513Z",
     "start_time": "2022-11-18T14:52:34.006613Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_img(dir_path):\n",
    "    \"\"\"\n",
    "    Load all .jpg images saved in a directory to a binary Spark DataFrame.\n",
    "    \"\"\"\n",
    "    images = spark.read.format(\"binaryFile\") \\\n",
    "        .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "        .option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .load(dir_path)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "397f29f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:34.939544Z",
     "start_time": "2022-11-18T14:52:34.846242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "dataframes =[load_img(dir_path) for dir_path in subfolders]\n",
    "\n",
    "# merge DataFrames\n",
    "image_df = reduce(lambda first, second: first.union(second), dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36e9e6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:35.664939Z",
     "start_time": "2022-11-18T14:52:35.662232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ac2f2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T22:01:50.712027Z",
     "start_time": "2022-11-17T22:01:48.978147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load images\n",
    "dataframes =[spark.read.format('image').load(p) for p in subfolders]\n",
    "\n",
    "# merge DataFrames\n",
    "image_df = reduce(lambda first, second: first.union(second), dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc72791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-17T22:01:50.722901Z",
     "start_time": "2022-11-17T22:01:50.713534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796d0a7",
   "metadata": {},
   "source": [
    "## Create a Category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "389f5cc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:52:38.988453Z",
     "start_time": "2022-11-18T14:52:38.798807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+------------+\n",
      "|                path|   modificationTime|length|             content|    category|\n",
      "+--------------------+-------------------+------+--------------------+------------+\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4869|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4865|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4857|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4847|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4847|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4842|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4834|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4824|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4820|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "|file:/Users/pierr...|2021-09-12 19:25:46|  4815|[FF D8 FF E0 00 1...|Apple_Golden|\n",
      "+--------------------+-------------------+------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex = r'(.*)/(.*[a-zA-Z])(.*)/'\n",
    "df = image_df.withColumn('category', regexp_extract('path', regex, 2))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93782b02",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba536e",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72b882",
   "metadata": {},
   "source": [
    "We will use the **InceptionV3** model to extract features from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf1087b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:21:48.429456Z",
     "start_time": "2022-11-18T14:21:48.186883Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparkdl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparkdl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepImageFeaturizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model: InceptionV3\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# extracting feature from images\u001b[39;00m\n\u001b[1;32m      5\u001b[0m featurizer \u001b[38;5;241m=\u001b[39m DeepImageFeaturizer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                  outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                                  modelName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInceptionV3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparkdl'"
     ]
    }
   ],
   "source": [
    "from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "# model: InceptionV3\n",
    "# extracting feature from images\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                                 outputCol=\"features\",\n",
    "                                 modelName=\"InceptionV3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1600827",
   "metadata": {},
   "source": [
    "We will use **Broadcast variables** to reduce communication costs.  \n",
    "Broadcast variables are read-only shared variables, they are cached and available on all nodes in a cluster to be accessed or used by the tasks. Instead sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6182c19c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:53:52.218247Z",
     "start_time": "2022-11-18T14:53:50.137652Z"
    }
   },
   "outputs": [],
   "source": [
    "model = InceptionV3(include_top=False)\n",
    "#model = ResNet50(include_top=False)\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed\n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = InceptionV3(weights=None, include_top=False)\n",
    "    model.set_weights(bc_model_weights.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c73a3",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8d7a858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:53:57.430579Z",
     "start_time": "2022-11-18T14:53:57.427865Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_img(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7237f",
   "metadata": {},
   "source": [
    "### Define featurization in a Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42bbb6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:53:58.883807Z",
     "start_time": "2022-11-18T14:53:58.880854Z"
    }
   },
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    For some layers, output features will be multi-dimensional tensors.\n",
    "    Feature tensors are flattened to vectors for easier storage in Spark DataFrames.\n",
    "    -----------    \n",
    "    Return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    arr = np.stack(content_series.map(preprocess_img))\n",
    "    preds = model.predict(arr)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9aace0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:54:02.422981Z",
     "start_time": "2022-11-18T14:54:02.329436Z"
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    \"\"\"\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns\n",
    "    a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    With Sclaar Iterator pandas UDFS,\n",
    "    the model can be loaded once and re-used after for multiple data batches.\n",
    "    This amortizes the overhead of loading big models.\n",
    "    -----------\n",
    "    Parameters:\n",
    "    content_series_iter: This argument is an iterator over batches of data,\n",
    "                         where each batch is a pandas Series of image data.\n",
    "    -----------\n",
    "    Return: a Spark DataFrame column of type ArrayType(FloatType)\n",
    "    \"\"\"\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5141c7",
   "metadata": {},
   "source": [
    "### Apply featurization to the DataFrame of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5da10",
   "metadata": {},
   "source": [
    "Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors. It can be avoided by reducing the Arrow batch size via `maxRecordsPerBatch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b29aa59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:54:05.123617Z",
     "start_time": "2022-11-18T14:54:05.121209Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "27303cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T14:54:13.776039Z",
     "start_time": "2022-11-18T14:54:06.256997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:54:06.665196: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 15:54:09.647279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+\n",
      "|                path|    category|             content|            features|\n",
      "+--------------------+------------+--------------------+--------------------+\n",
      "|file:/Users/pierr...|Apple_Golden|[FF D8 FF E0 00 1...|[0.0, 0.29480368,...|\n",
      "|file:/Users/pierr...|Apple_Golden|[FF D8 FF E0 00 1...|[0.0, 0.4715738, ...|\n",
      "|file:/Users/pierr...|Apple_Golden|[FF D8 FF E0 00 1...|[0.0, 0.30836552,...|\n",
      "|file:/Users/pierr...|Apple_Golden|[FF D8 FF E0 00 1...|[0.0, 0.1550447, ...|\n",
      "|file:/Users/pierr...|Apple_Golden|[FF D8 FF E0 00 1...|[0.0, 0.28061968,...|\n",
      "+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df = df.withColumn('features', featurize_udf('content')) \\\n",
    "                .select('path', 'category', 'content', 'features')\n",
    "\n",
    "features_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d6c06",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede2c78",
   "metadata": {},
   "source": [
    "Results will be saved using the parquet format for performance purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2142ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(\"path_s3_buquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
